{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207838a9",
   "metadata": {},
   "source": [
    "# Intro to Gradient Descent (and computational science)\n",
    "\n",
    "**Prepared by Dan Hackett (2025)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77676981",
   "metadata": {},
   "source": [
    "This exercise is an introduction to computational science using Python and Jupyter notebooks, and secretly also an introduction to coding if you haven't done much before. The best way to learn how to use a tool is to try to use it to do something. So, as a well-motivated excuse to learn, we're going to implement \"gradient descent\", the backbone algorithm underlying all of training in AI/ML.\n",
    "\n",
    "Gradient descent is an *optimization algorithm* or *optimizer*. Its objective is to find the parameters which minimize a \"loss function\".\n",
    "\n",
    "Gradient descent is easy to understand from a physics analogy: we're just going to simulate something sliding down a hill to the lowest point it can find. In the analogy, the \"loss function\" is just the height of the hill as a function of the position $x$. We'll call this function $V(x)$.\n",
    "\n",
    "**Note:** gradient descent isn't *quite* the same as e.g. a rollercoaster on a track or a ball rolling down a hill. That's because in gradient descent, the thing that's moving doesn't have any momentum. So, it won't pick up speed as it goes downhill the way a rolling ball would. Instead, the \"speed\" it moves with just depends on how steep the hill is. It's more like something sliding with a lot of friction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd3643",
   "metadata": {},
   "source": [
    "Here's an example:\n",
    "$$\n",
    "    V(x) = -0.1x - x^2 + x^4\n",
    "$$\n",
    "We'll work with this one all through the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d464d68e",
   "metadata": {},
   "source": [
    "# Plotting V(x) the hard way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c914bd0",
   "metadata": {},
   "source": [
    "First, we're going to slowly build up to making a plot of $V(x)$ vs $x$. In this first section, we'll do things the hard way to review some basic programming concepts. Later on, we'll see how more powerful tools (specifically, numpy) can make this much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96202072",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Okay, let's get started.\n",
    "\n",
    "**Write a line of Python code which computes $V(x)$.** We've set the variable `x` to a value already; write your code to compute $V(x)$ from this variable.\n",
    "\n",
    "**HINT:** in Python, you raise things to a power by using `**`, not `^`. So, to compute $x^2$, you do `x**2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "V = ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48828aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to check your answer numerically\n",
    "import numpy as np\n",
    "assert np.isclose(V, -0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "x = 1\n",
    "V = -0.1*x - x**2 + x**4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97b65b",
   "metadata": {},
   "source": [
    "Now, you have some code that should be able to compute $V(x)$ for any arbitrary value of $x$. But we need to check that it's right!\n",
    "\n",
    "To check your expression for `V`, we want to use it to make a plot of $V(x)$ as a function of $x$ so we can see whether it looks right. To do that, we first need to evaluate $V(x)$ on a grid of different values of $x$. We'll walk through how to do this a few different ways.\n",
    "\n",
    "First, let's explore how to do this using `list`s, `for` loops, and the \"accumulator\" pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26810b86",
   "metadata": {},
   "source": [
    "## Review: Collections and `list`\n",
    "\n",
    "Let's see how to work with `list`, one of Python's basic built-in collections. A collection is an object which contains other objects. A `list` (linked list) stores an ordered list of objects. In Python, they are denoted with square brackets and commas like `[3,1,2]`. An empty list is `[]`.\n",
    "\n",
    "Run the cells below to see how lists work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d40def",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [3,1,2] # make a new list with three items\n",
    "print(x)\n",
    "x.append(7) # add an item to the list\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae59196",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [] # make an empty list\n",
    "x.append(1) # add an item\n",
    "print(x)\n",
    "x.append(5) # add an item\n",
    "print(x)\n",
    "\n",
    "x = [6] # make a new list, overwrite old one\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4762e8d",
   "metadata": {},
   "source": [
    "## Review: `for` loops, range\n",
    "\n",
    "`for` loops let you run the same block of code repeatedly, except with some variable which changes its value in each \"iteration\" of the loop, so it doesn't just do exactly the same thing every time.\n",
    "\n",
    "In Python, `for` loops run over all the variables in a collection, so you can think of them as running code \"for each\" item in the collection in order.\n",
    "\n",
    "Play with the example code below to see what a `for` loop does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328fac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [5,3,7]: # try adding or removing items from the list\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee59c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [4,2,1]\n",
    "for x in a_list:\n",
    "    x = x+1\n",
    "    print(x)\n",
    "print(a_list) # doesn't modify items in original list!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9866e",
   "metadata": {},
   "source": [
    "Python has a built-in helper function `range` to do `for` loops over sequential integers. For example, `range(5)` lets you do a loop over `0,1,2,3,4`. Note that it doesn't include the last integer in the sequence.\n",
    "\n",
    "Play with the example code below to see what `range` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10): # try changing 10 to other numbers\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(-5,5): # can start from values other than zero\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d16b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(-10,10,2): # bigger steps\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff4f2c",
   "metadata": {},
   "source": [
    "## Concept: accumulator pattern\n",
    "\n",
    "A useful way to combine `for` and `list` is the \"accumulator\" pattern. This lets us build up (accumulate) a list full of values following a certain pattern.\n",
    "\n",
    "Play with the code below to understand the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a099c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "for i in range(5):\n",
    "    x = 3 * i**2 + 1\n",
    "    a_list.append(x)\n",
    "print(a_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff1fe3",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Now, let's use this pattern to make a list of values of $x$. Specifically, we want 101 evenly-spaced values between -1 and 1. The first value should be -1, the last value should be 1.\n",
    "\n",
    "**NOTE:** you might see some round-off error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b24661",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = []\n",
    "\n",
    "# TODO\n",
    "\n",
    "print(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "all_x = []\n",
    "for i in range(101):\n",
    "    x = -1 + i/50\n",
    "    all_x.append(x)\n",
    "print(all_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d503cf9",
   "metadata": {},
   "source": [
    "Let's check whether it worked. Often it's useful to make a quick plot just to see whether your code ran correctly. \n",
    "\n",
    "**Run the code below and see if it makes sense!** This should also remind you how to make a simple plot with matplotlib.\n",
    "\n",
    "**NOTE:** if you just call `plt.plot` with one argument like `plt.plot(x)`, it will plot the values in `x` on the y-axis versus `0,1,...,len(x)-1` on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_x, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29016440",
   "metadata": {},
   "source": [
    "We can also check the values directly. Run the code below to check if you got it right.\n",
    "\n",
    "If the checks below all pass, then you've definitely got the right answer. Can you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "assert all_x[0] == -1 # check first item\n",
    "assert all_x[-1] == 1 # check last item\n",
    "# even spacing\n",
    "dx = all_x[1] - all_x[0]\n",
    "for i in range(100):\n",
    "    # This code won't work because of round-off error:\n",
    "    #assert (all_x[i+1] - all_x[i]) == dx\n",
    "    # Use helper function from numpy for floating-point comparisons instead:\n",
    "    assert np.isclose((all_x[i+1] - all_x[i]), dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e0000",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Now, let's put it all together! Go back and grab your code from above and use it to **write a new accumulator pattern which computes $V(x)$** for each one of these values of $x$, and stores it in a list `all_V`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef40064",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_V = []\n",
    "\n",
    "# TODO (hint: do some copy-pasting!)\n",
    "\n",
    "print(all_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cec4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "all_V = []\n",
    "for x in all_x:\n",
    "    V = -0.1*x - x**2 + x**4\n",
    "    all_V.append(V)\n",
    "print(all_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743adf69",
   "metadata": {},
   "source": [
    "Now, run the code below to make a plot of $V(x)$ vs $x$. **Does this look right?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_x, all_V, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98f51f",
   "metadata": {},
   "source": [
    "# Plotting V(x) the easy way\n",
    "\n",
    "That was a lot of work just to make one plot! Fortunately, there are much faster and easier ways to do all this using Python's standard NumPy library (always abbreviated as `np`).\n",
    "\n",
    "If we hadn't already imported numpy for some of the tests above, you'd run the cell below to import numpy in the standard way. Running it again is unnecessary, but won't break anything (or do anything at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e079f4f",
   "metadata": {},
   "source": [
    "## Review: numpy\n",
    "\n",
    "NumPy is a library for creating and manipulating N-dimensional arrays (`NDArray`s) of numbers. There are many ways to create numpy arrays, for example by passing a list to the `np.array` function.\n",
    "\n",
    "Array operations are *broadcasted*. That means that any math operation gets applied to every element in the array.\n",
    "\n",
    "**Play with the code below to see how this works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3]) # make an array from a list\n",
    "print(x)\n",
    "\n",
    "x = np.arange(5) # like range, but makes a numpy array\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4436c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3])\n",
    "print(2*x) # multiply every number by 2\n",
    "print(x**2) # square every number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ab0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operations involving two different arrays\n",
    "x = np.array([3,1,2])\n",
    "y = np.array([2,3,5])\n",
    "print(x*y)\n",
    "print(x+y)\n",
    "print(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26aaaa3",
   "metadata": {},
   "source": [
    "One very useful helper function is `np.linspace`, which makes an evenly-spaced grid of values.\n",
    "\n",
    "**Run the cell below for an example.** How does this compare with `all_x` we computed above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890753c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,101)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1492ddd",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "**Now, try taking this `x` made by running `linspace` and running your code to compute `V`.** What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,101)\n",
    "\n",
    "V = # TODO: copy-paste your code from exercise 1\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003abba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "x = np.linspace(-1,1,1024)\n",
    "V = -0.1*x - x**2 + x**4\n",
    "print(x,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9334a",
   "metadata": {},
   "source": [
    "**Now, use these to make a plot of $V$ vs $x$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(#TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c20c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "plt.plot(x,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf5aac",
   "metadata": {},
   "source": [
    "# Packaging up V(x)\n",
    "\n",
    "By now, you should have a bit of code that can compute $V(x)$ correctly. But, you don't want to have to copy-paste this everywhere---this is a very easy way to make mistakes! Instead, we're going to package your code into a function that we can call later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96c0ef",
   "metadata": {},
   "source": [
    "## Review: functions\n",
    "\n",
    "A function is a bundle of code that you can run over and over with different inputs.\n",
    "\n",
    "Functions aren't exactly the same in code as in math. Like functions in math, they can evaluate to (return) a value which depends on their arguments.\n",
    "\n",
    "The code \"inside\" a function is an indented block, just like with `if`, `while`, and `for`. This code runs whenever you call the function.\n",
    "\n",
    "**Play with the code below to remember how defining a function works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d4402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a,b):\n",
    "    y = a*b # run some code\n",
    "    return y # return value that we computed\n",
    "print(multiply(2,4))\n",
    "print(multiply(9,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca643f0",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Write a function to compute $V(x)$ named `calc_V` which takes a single argument `x` and returns $V(x)$ for that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_V(x):\n",
    "    # TODO (HINT: copy-paste your code here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57069f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def calc_V(x):\n",
    "    return -0.1*x - x**2 + x**4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c55b44",
   "metadata": {},
   "source": [
    "Now, **call `calc_V` to compute $V$, and make a plot of $V(x)$ vs $x$** to check everything worked okay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,101)\n",
    "\n",
    "V = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf828b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(#TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "x = np.linspace(-1,1,101)\n",
    "V = calc_V(x)\n",
    "plt.plot(x, V, '.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa6610",
   "metadata": {},
   "source": [
    "# Derivatives and finite differences\n",
    "\n",
    "One last concept!\n",
    "\n",
    "In order to implement gradient descent, we don't just need $V(x)$, we also need its derivative. This gets notated like $V'(x)$ if you are Newton, or $\\frac{\\partial V}{\\partial x}$ if you are Leibniz. In case you don't remember what a derivative is: $V'(x)$ is just the slope right at the point $x$.\n",
    "\n",
    "We're going to take the derivative the pencil-and-paper way below, but first, let's make the computer do it the hard way. This will also give us something to check our pen-and-paper answer below.\n",
    "\n",
    "Derivatives are easy to approximate numerically as a \"finite difference\":\n",
    "$$\n",
    "    V'(x) = \\frac{dV}{dx} \\approx \\frac{V(x+\\epsilon) - V(x)}{\\epsilon}\n",
    "$$\n",
    "This just computes the slope by comparing $V(x)$ with its value a small step $\\epsilon$ away. If we take $\\epsilon \\rightarrow 0$, it becomes $V'(x)$ exactly. (But if we're doing it numerically, we have to pick some small but nonzero $\\epsilon$!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b652936",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "**Compute a finite difference approximation of $V'(x)$ on a grid of $x$ and make a plot of it.**\n",
    "\n",
    "**Note:** there are many ways to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1143887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0700b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION (e.g.)\n",
    "x = np.linspace(-1,1,100)\n",
    "V = calc_V(x)\n",
    "dVdx = np.diff(V) / np.diff(x)\n",
    "plt.plot(x[:-1], dVdx, '.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71666f98",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Fortunately, thanks to the miracle of calculus, we can just take the derivative analytically and get a new function we can evaluate separately. Recall that our function is:\n",
    "$$\n",
    "    V(x) = -0.1x - x^2 + x^4\n",
    "$$\n",
    "Its derivative is just:\n",
    "$$\n",
    "V'(x) = \\frac{\\partial V}{\\partial x} = -0.1 - 2x + 4x^3\n",
    "$$\n",
    "\n",
    "Now, **write a function `calc_dVdx` that takes a single argument `x` and returns `V'(x)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5326c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dVdx(x):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def calc_dVdx(x):\n",
    "    return -0.1 - 2*x + 4*x**3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72caab",
   "metadata": {},
   "source": [
    "**Call `calc_dVdx` to compute $V'(x)$ on a grid and make a plot of $V'(x)$ vs $x$.** Does it look right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,101)\n",
    "\n",
    "dVdx = # TODO\n",
    "\n",
    "plt.plot(#TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "x = np.linspace(-1,1,101)\n",
    "V = calc_V(x)\n",
    "dVdx = calc_dVdx(x)\n",
    "plt.plot(x, V)\n",
    "plt.plot(x, dVdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b7470",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "Now, we've implemented $V'(x)$ two different ways. Let's make sure we get the same answer from both! If they don't agree, then at least one of them must be wrong.\n",
    "\n",
    "**Make a plot with your two different versions of `V'` on it.** Do they look the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: calling plt.plot twice will put two lines on the same plot\n",
    "plt.plot(#TODO: finite different version)\n",
    "plt.plot(#TODO: analytic version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "x = np.linspace(-1,1,101)\n",
    "V = calc_V(x)\n",
    "dVdx = calc_dVdx(x)\n",
    "plt.plot(x, dVdx)\n",
    "plt.plot(x[:-1], np.diff(V)/np.diff(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f035f",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "Okay, now we have all the ingredients we need to implement gradient descent!\n",
    "\n",
    "The steps of the gradient descent algorithm are:\n",
    "1. Pick some starting value $x$\n",
    "2. Take a step: update $x$ according to the rule\n",
    "$$\n",
    "    x \\rightarrow x - \\eta V'(x)\n",
    "$$\n",
    "where $\\eta$ is a \"hyperparameter\" called the step size.\n",
    "3. Repeat step 2 many times until we're at a minimum.\n",
    "\n",
    "We said before the gradient descent is like something sliding down a hill. You can think of this rule as defining how its position $x$ evolves in time, like $x(t)$. Each iteration of gradient descent is like a fixed step forward in time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c40fa7d",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "It's a good idea to write your code \"from the inside out\". So, before we get to running many steps, let's just write code to run one step.\n",
    "\n",
    "**Write the code to update the value of $x$ (step 2 above).** Use the variable `step_size` for $\\eta$. \n",
    "\n",
    "**HINT:** you'll need to call your $V'(x)$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cd3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "step_size = 0.1 # \"eta\" by another name\n",
    "\n",
    "print(x)\n",
    "\n",
    "# TODO: update x\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "x = 0\n",
    "step_size = 0.1 # \"eta\" by another name\n",
    "\n",
    "print(x)\n",
    "dVdx = calc_dVdx(x)\n",
    "x = x - step_size * dVdx\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe07269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your answer!\n",
    "assert np.isclose(x, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8f8ea",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "\n",
    "Now, on to actually implementing it! What you need to do is:\n",
    "* Pick some starting value for `x`\n",
    "* Make a for loop to run 100 steps\n",
    "\n",
    "Inside the loop, \n",
    "* Update `x`\n",
    "* Save the new value of `x` inside a list named `history`\n",
    "\n",
    "At the end, we should have a list `history` that tells us how `x` evolved under gradient descent. `history` is like $x(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "step_size = 0.05\n",
    "\n",
    "# TODO: set initial x value\n",
    "\n",
    "# TODO: write the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "history = []\n",
    "step_size = 0.05\n",
    "\n",
    "x = -0.5 # starting value\n",
    "\n",
    "for n in range(100):\n",
    "    dVdx = calc_dVdx(x)\n",
    "    x = x - step_size * dVdx\n",
    "    history.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa465099",
   "metadata": {},
   "source": [
    "Now, run the first cell below to make a plot of $V(x)$ vs $x$ (for reference/comparison), and the second cell to make a plot of how $x(t)$ evolved under gradient descent.\n",
    "\n",
    "Does the value $x(t)$ goes towards make sense? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,1024)\n",
    "V = calc_V(x)\n",
    "plt.plot(x, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8024d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312bdd9",
   "metadata": {},
   "source": [
    "## Exercise 11\n",
    "\n",
    "Now we're going to look at all the different ways that gradient descent can break! This will be more of a free-form exploration.\n",
    "\n",
    "You may have noticed that it went to the \"wrong minimum\" out of the two. This is a local minimum, and not the global minimum where the loss is actually minimized. This is a well-known failure mode of gradient descent! Let's see if you can fix it.\n",
    "\n",
    "The knobs you can turn are the starting value of $x$ and the step size. Try:\n",
    "* Make the step size smaller and smaller.\n",
    "* Make the step size larger and larger.\n",
    "* Picking different values of starting $x$.\n",
    "What happens in each case?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: it can be a good idea to collect all the code in one cell here,\n",
    "# so that you don't have to scroll up and rerun a bunch of cells to play around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c72755",
   "metadata": {},
   "source": [
    "# Bonus: Stochastic gradient descent\n",
    "\n",
    "In real life ML, we won't be able to compute $V'(x)$ exactly. Instead, we only get to compute a noisy approximation of it using whatever data we have handy (e.g. the entire internet). If you only have a noisy estimator for $V'(x)$, then you are doing \"stochastic gradient descent\" (SGD) instead of just the normal kind.\n",
    "\n",
    "We can make a toy model of SGD very easily. Numpy has built-in random number generators you can use. The cell below draws a Gaussian random number (i.e., a number taken from the normal distribution or \"bell curve\"). Run it a couple times and convince yourself it's random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c28b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.normal(0, 0.1) # first parameter is the mean, second parameter is standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41747999",
   "metadata": {},
   "source": [
    "To emulate SGD, we can just take `calc_dVdx` from early and add a little bit of random noise to it.\n",
    "\n",
    "**Run the cell below.** This function is a \"wrapper\" around your `calc_dVdx` from earlier. It'll compute $V'(x)$, add a bit of random noise to it, then return the noised-up value. `sigma` is the standard deviation of the noise; passing `sigma=0` won't add any noise, passing `sigma=10000` will add a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_noisy_dVdx(x, sigma=0.5):\n",
    "    dVdx = calc_dVdx(x)\n",
    "    dVdx += np.random.normal(0,sigma)\n",
    "    return dVdx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca3ca0",
   "metadata": {},
   "source": [
    "## Exercise 12\n",
    "\n",
    "Now, take your gradient descent implementation from above, copy-paste it down here, and change where it calls `calc_dVdx` to call `calc_noisy_dVdx` instead. Run it and see what happens!\n",
    "\n",
    "Once you've got it working, try playing around with the noise level `sigma` (you can pass this as an extra argument to `calc_noisy_dVdx`) and the other parameters. Some things to think about:\n",
    "* SGD won't ever settle exactly into a minimum, just bounce around. Any ideas how you could fix this?\n",
    "* What happens if you make the noise level too high?\n",
    "* SGD can hop between the different minima in $V(x)$. Try to get this to happen!\n",
    "\n",
    "If you get frustrated that the noise changes every time you run the cell, use `np.random.seed` to seed the RNG before you run your loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: gradient descent, but with calc_dVdx -> calc_noisy_dVdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "history = []\n",
    "step_size = 0.05\n",
    "\n",
    "x = 0.1\n",
    "for n in range(100):\n",
    "    dVdx = calc_noisy_dVdx(x, sigma=1)\n",
    "    x = x - step_size * dVdx\n",
    "    history.append(x)\n",
    "    \n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db42504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for fun: learning rate scheduling!\n",
    "history = []\n",
    "start_x = 0.1\n",
    "step_size = 0.05\n",
    "\n",
    "x = start_x\n",
    "for n in range(100):\n",
    "    dVdx = calc_noisy_dVdx(x)\n",
    "    x = x - step_size * dVdx\n",
    "    if n%10 == 0:\n",
    "        step_size /= 2\n",
    "    history.append(x)\n",
    "    \n",
    "plt.plot(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
